{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xctusmJ6BZ6_"
      },
      "source": [
        "# Scaling Test-Time Compute for Longer Thinking in LLMs\n",
        "\n",
        "_Authored by: [Sergio Paniego](https://github.com/sergiopaniego)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmgoppItAO7B"
      },
      "source": [
        "ğŸš¨ **WARNING**: This notebook is **resource-intensive** and requires substantial computational power. If youâ€™re running this in **Colab**, it will utilize an **A100 GPU**.\n",
        "\n",
        "---\n",
        "\n",
        "In this recipe, we'll guide you through extending the inference time for an **Instruct LLM system** using **test-time compute** to solve more challenging problems, such as **complex math problems**. This approach, inspired by [**OpenAI o1-o3 models**](https://openai.com/index/learning-to-reason-with-llms/), demonstrates that **longer reasoning time** during inference can enhance model performance.\n",
        "\n",
        "This technique builds on experiments shared in [this **blog post**](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute), which show that smaller models, like the **1B** and **3B Llama Instruct models**, can outperform much larger ones on the **MATH-500 benchmark** when given enough **\"time to think\"**. Recent research from [DeepMind](https://arxiv.org/abs/2408.03314) suggests that **test-time compute** can be scaled optimally through strategies like iterative self-refinement or using a reward model.\n",
        "\n",
        "The blog introduces a [**new repository**](https://github.com/huggingface/search-and-learn) for running these experiments. In this recipe, we'll focus on building a **small chatbot** that engages in **longer reasoning** to tackle **harder problems** using small open models.\n",
        "\n",
        "![Instruct LLM Methodology](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/methods-thumbnail.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twKCzVIg71Xa"
      },
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Letâ€™s start by installing the [search-and-learn](https://github.com/huggingface/search-and-learn) repository! ğŸš€  \n",
        "This repo is designed to replicate the experimental results and is not a Python pip package. However, we can still use it to generate our system. To do so, weâ€™ll need to install it from source with the following steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0YDC2_7XTm8",
        "outputId": "9128f1d7-edac-4db6-e67f-c0198540f180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'search-and-learn'...\n",
            "remote: Enumerating objects: 352, done.\u001b[K\n",
            "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 352 (delta 104), reused 82 (delta 82), pack-reused 194 (from 2)\u001b[K\n",
            "Receiving objects: 100% (352/352), 942.98 KiB | 20.95 MiB/s, done.\n",
            "Resolving deltas: 100% (168/168), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/search-and-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT3jH_d_XcEb",
        "outputId": "0a83d830-ec68-4abc-cbb6-0482c4f3a98b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/search-and-learn\n",
            "Obtaining file:///content/search-and-learn\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from search-and-learn==0.1.0) (1.6.0)\n",
            "Collecting pebble (from search-and-learn==0.1.0)\n",
            "  Downloading pebble-5.1.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting latex2sympy2==1.9.1 (from search-and-learn==0.1.0)\n",
            "  Downloading latex2sympy2-1.9.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting word2number (from search-and-learn==0.1.0)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.47.0 in /usr/local/lib/python3.11/dist-packages (from search-and-learn==0.1.0) (4.51.3)\n",
            "Collecting fastapi (from search-and-learn==0.1.0)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting hf_transfer (from search-and-learn==0.1.0)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: sympy>=1.4 in /usr/local/lib/python3.11/dist-packages (from latex2sympy2==1.9.1->search-and-learn==0.1.0) (1.13.1)\n",
            "Collecting antlr4-python3-runtime==4.7.2 (from latex2sympy2==1.9.1->search-and-learn==0.1.0)\n",
            "  Downloading antlr4-python3-runtime-4.7.2.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting vllm==0.6.3 (from search-and-learn==0.1.0)\n",
            "  Downloading vllm-0.6.3-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting ruff (from search-and-learn==0.1.0)\n",
            "  Downloading ruff-0.11.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting isort (from search-and-learn==0.1.0)\n",
            "  Downloading isort-6.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from search-and-learn==0.1.0) (8.3.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.2.0)\n",
            "Collecting numpy<2.0.0 (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.21.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (5.29.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (3.11.15)\n",
            "Requirement already satisfied: openai>=1.40.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (1.76.0)\n",
            "Collecting uvicorn[standard] (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (2.11.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (11.2.1)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.21.1)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken>=0.6.0 (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting lm-format-enforcer==0.10.6 (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading lm_format_enforcer-0.10.6-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting outlines<0.1,>=0.0.43 (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (4.13.2)\n",
            "Requirement already satisfied: filelock>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (3.18.0)\n",
            "Collecting partial-json-parser (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (24.0.1)\n",
            "Collecting msgspec (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf==0.10.0 (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (8.6.1)\n",
            "Collecting mistral-common>=1.4.4 (from mistral-common[opencv]>=1.4.4->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (0.8.1)\n",
            "Collecting ray>=2.9 (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.3->search-and-learn==0.1.0) (12.570.86)\n",
            "Collecting torch==2.4.0 (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.19 (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting xformers==0.0.27.post2 (from vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading xformers-0.0.27.post2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.10.6->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer==0.10.6->vllm==0.6.3->search-and-learn==0.1.0) (24.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (12.5.82)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi->search-and-learn==0.1.0)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.47.0->search-and-learn==0.1.0) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.47.0->search-and-learn==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.47.0->search-and-learn==0.1.0) (0.5.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->search-and-learn==0.1.0) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->search-and-learn==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm==0.6.3->search-and-learn==0.1.0) (4.23.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common[opencv]>=1.4.4->vllm==0.6.3->search-and-learn==0.1.0) (4.11.0.86)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (1.3.1)\n",
            "Collecting lark (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (3.1.1)\n",
            "Collecting diskcache (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (0.60.0)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (0.36.2)\n",
            "Collecting datasets (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pycountry (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyairports (from outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading pyairports-2.1.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm==0.6.3->search-and-learn==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm==0.6.3->search-and-learn==0.1.0) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm==0.6.3->search-and-learn==0.1.0) (0.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.6.3->search-and-learn==0.1.0) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.6.3->search-and-learn==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.6.3->search-and-learn==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.6.3->search-and-learn==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.6.3->search-and-learn==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.6.3->search-and-learn==0.1.0) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.4->latex2sympy2==1.9.1->search-and-learn==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.3->search-and-learn==0.1.0) (1.20.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->vllm==0.6.3->search-and-learn==0.1.0) (3.21.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0) (0.16.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.6.3->search-and-learn==0.1.0) (15.0.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.40.0->vllm==0.6.3->search-and-learn==0.1.0) (1.0.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm==0.6.3->search-and-learn==0.1.0) (2025.4.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm==0.6.3->search-and-learn==0.1.0) (0.24.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (2.2.2)\n",
            "Collecting xxhash (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0->vllm==0.6.3->search-and-learn==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3->search-and-learn==0.1.0) (1.17.0)\n",
            "Downloading latex2sympy2-1.9.1-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.8/89.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vllm-0.6.3-cp38-abi3-manylinux1_x86_64.whl (193.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.5/193.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.10.6-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.27.post2-cp311-cp311-manylinux2014_x86_64.whl (20.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isort-6.0.1-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pebble-5.1.1-py3-none-any.whl (34 kB)\n",
            "Downloading ruff-0.11.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m119.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines-0.0.46-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl (68.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.4/68.4 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyairports-2.1.1-py3-none-any.whl (371 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: search-and-learn, antlr4-python3-runtime, word2number\n",
            "  Building editable for search-and-learn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for search-and-learn: filename=search_and_learn-0.1.0-0.editable-py3-none-any.whl size=8676 sha256=dc519157556fee1ca5213374f9bc1d0c0298631cd30ed0858622a69045d05b0e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o0xs8qto/wheels/87/cf/59/86a78cb0faa390ac87785f5d31799a5587b8e52dc5308682f2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.7.2-py3-none-any.whl size=140930 sha256=25c3b4d951215602183dcb9c25cc2e58a42fe139a7ca919f7115006f8a01912e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/f4/c7/71f768e561b9cc8bce3160d1229728d0d77fd5fd6d4f5d960b\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=93560d88f2876f061967b17571eb4a6bd85d2c2395bf08440fffc719a8d2c38f\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built search-and-learn antlr4-python3-runtime word2number\n",
            "Installing collected packages: word2number, pyairports, antlr4-python3-runtime, xxhash, uvloop, uvicorn, triton, ruff, python-dotenv, pycountry, pebble, partial-json-parser, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, msgspec, lark, isort, interegular, httptools, hf_transfer, fsspec, diskcache, dill, watchfiles, tiktoken, starlette, nvidia-cusolver-cu12, nvidia-cudnn-cu12, multiprocess, latex2sympy2, gguf, torch, prometheus-fastapi-instrumentator, lm-format-enforcer, fastapi, xformers, torchvision, ray, mistral-common, datasets, search-and-learn, outlines, vllm\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n"
          ]
        }
      ],
      "source": [
        "%cd search-and-learn\n",
        "!pip install -e '.[dev]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAQHu9T176zh"
      },
      "source": [
        "Log in to Hugging Face to access [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct), as it is a gated model! ğŸ—ï¸  \n",
        "If you haven't previously requested access, you'll need to submit a request before proceeding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnEaTlFYZF_H"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX07zCTA8MWL"
      },
      "source": [
        "## 2. Setup the Large Language Model (LLM) and the Process Reward Model (PRM) ğŸ’¬\n",
        "\n",
        "As illustrated in the diagram, the system consists of an LLM that generates intermediate answers based on user input, a [PRM model](https://huggingface.co/papers/2211.14275) that evaluates and scores these answers, and a search strategy that uses the PRM feedback to guide the subsequent steps in the search process until reaching the final answer.\n",
        "\n",
        "Letâ€™s begin by initializing each model. For the LLM, weâ€™ll use the [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) model, and for the PRM, weâ€™ll use the [RLHFlow/Llama3.1-8B-PRM-Deepseek-Data](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data) model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkJw0x7gDJEY"
      },
      "source": [
        "![system](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/system.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG1MolfxmZ7M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from vllm import LLM\n",
        "from sal.models.reward_models import RLHFFlow\n",
        "\n",
        "model_path=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "prm_path=\"RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\"\n",
        "\n",
        "llm = LLM(\n",
        "    model=model_path,\n",
        "    gpu_memory_utilization=0.5,  # Utilize 50% of GPU memory\n",
        "    enable_prefix_caching=True,  # Optimize repeated prefix computations\n",
        "    seed=42,                     # Set seed for reproducibility\n",
        ")\n",
        "\n",
        "prm = RLHFFlow(prm_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYtPn0_V_YRx"
      },
      "source": [
        "### 2.1 Instantiate the Question, Search Strategy, and Call the Pipeline\n",
        "\n",
        "Now that we've set up the LLM and PRM, let's proceed by defining the question, selecting a search strategy to retrieve relevant information, and calling the pipeline to process the question through the models.\n",
        "\n",
        "1. **Instantiate the Question**: In this step, we define the input question that the system will answer, considering the given context.\n",
        "\n",
        "2. **Search Strategy**: The system currently supports the following search strategies: `best_of_n`, `beam_search`, and `dvts` (see diagram). For this example, we'll use `best_of_n`, but you can easily switch to any of the other strategies based on your needs. We need to define some configuration parameters for the configuration of the search strategy. You can check the full list [here](https://github.com/huggingface/search-and-learn/blob/main/src/sal/config.py).\n",
        "\n",
        "3. **Call the Pipeline**: With the question and search strategy in place, weâ€™ll call the inference pipeline, processing the inputs through both the LLM and PRM to generate the final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSWINPerJrhm"
      },
      "source": [
        "![](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/search-strategies.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z69xD6i2L5a6"
      },
      "source": [
        "The first step is to clearly define the question that the system will answer. This ensures that we have a precise task for the model to tackle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83puLxhzsOM0"
      },
      "outputs": [],
      "source": [
        "question_text = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'\n",
        "input_batch = {\"problem\": [question_text]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGpyzMNkAO7H"
      },
      "source": [
        "Next, we define the configuration, including parameters like the number of candidate answers `(N)`, and choose the search strategy that will be used. The search strategy dictates how we explore the potential answers. In this case, we'll use `best_of_n`.\n",
        "\n",
        "With the question and configuration in place, we use the selected search strategy to generate multiple candidate answers. These candidates are evaluated based on their relevance and quality and the final answer is returned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6s6GS16QZLV"
      },
      "outputs": [],
      "source": [
        "from sal.config import Config\n",
        "from sal.search import beam_search, best_of_n, dvts\n",
        "\n",
        "config = Config()\n",
        "config.n=32 # Number of answers to generate during the search\n",
        "\n",
        "search_result = best_of_n(x=input_batch, config=config, llm=llm, prm=prm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsLHD_6C_15p"
      },
      "source": [
        "### 2.2 Display the Final Result\n",
        "\n",
        "Once the pipeline has processed the question through the LLM and PRM, we can display the final result. This result will be the model's output after considering the intermediate answers and scoring them using the PRM.\n",
        "\n",
        "Here's how to display the final answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "v8medbURbgdI",
        "outputId": "3620f3e6-a25d-4bec-f41c-c4f03a6ed770"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'## Step 1: Recall the conversion formulas\\nTo convert from rectangular coordinates $(x, y)$ to polar coordinates $(r, \\\\theta)$, we use the following formulas:\\n- $r = \\\\sqrt{x^2 + y^2}$\\n- $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{y}{x}\\\\right)$\\n\\n## Step 2: Substitute the given values into the formulas\\nGiven $(x, y) = (0, 3)$, we substitute these values into the formulas:\\n- $r = \\\\sqrt{0^2 + 3^2} = \\\\sqrt{0 + 9} = \\\\sqrt{9} = 3$\\n- $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{3}{0}\\\\right)$. However, since division by zero is undefined, we recognize that the point $(0, 3)$ is on the positive y-axis, meaning $\\\\theta = \\\\frac{\\\\pi}{2}$.\\n\\n## Step 3: Combine the results for the polar coordinates\\nTherefore, the polar coordinates of the point $(0, 3)$ are $\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)$.\\n\\nThe final answer is: $\\\\boxed{\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)}$'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search_result['pred'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-8hIu05AO7J"
      },
      "source": [
        "The modelâ€™s output might include special tokens, such as `<|start_header_id|>` or `<|end_header_id|>`. To make the answer more readable, we can safely remove them before displaying it to the end user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "flbIu6-rDapM",
        "outputId": "fcb197d5-0f21-4953-8a21-869c92a1f957"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'## Step 1: Recall the conversion formulas\\nTo convert from rectangular coordinates $(x, y)$ to polar coordinates $(r, \\\\theta)$, we use the following formulas:\\n- $r = \\\\sqrt{x^2 + y^2}$\\n- $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{y}{x}\\\\right)$\\n\\n## Step 2: Substitute the given values into the formulas\\nGiven $(x, y) = (0, 3)$, we substitute these values into the formulas:\\n- $r = \\\\sqrt{0^2 + 3^2} = \\\\sqrt{0 + 9} = \\\\sqrt{9} = 3$\\n- $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{3}{0}\\\\right)$. However, since division by zero is undefined, we recognize that the point $(0, 3)$ is on the positive y-axis, meaning $\\\\theta = \\\\frac{\\\\pi}{2}$.\\n\\n## Step 3: Combine the results for the polar coordinates\\nTherefore, the polar coordinates of the point $(0, 3)$ are $\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)$.\\n\\nThe final answer is: $\\\\boxed{\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)}$'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_output = search_result['pred'][0].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\").strip()\n",
        "formatted_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZuLZNirAO7J"
      },
      "source": [
        "After removing any special tokens, we can display the final answer to the user. Since the answer is based on markdown, it can be rendered properly by displaying it as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "P4En0qJRD0cl",
        "outputId": "56400fea-e304-4f16-d255-909f42f636e0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: Recall the conversion formulas\n",
              "To convert from rectangular coordinates $(x, y)$ to polar coordinates $(r, \\theta)$, we use the following formulas:\n",
              "- $r = \\sqrt{x^2 + y^2}$\n",
              "- $\\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)$\n",
              "\n",
              "## Step 2: Substitute the given values into the formulas\n",
              "Given $(x, y) = (0, 3)$, we substitute these values into the formulas:\n",
              "- $r = \\sqrt{0^2 + 3^2} = \\sqrt{0 + 9} = \\sqrt{9} = 3$\n",
              "- $\\theta = \\tan^{-1}\\left(\\frac{3}{0}\\right)$. However, since division by zero is undefined, we recognize that the point $(0, 3)$ is on the positive y-axis, meaning $\\theta = \\frac{\\pi}{2}$.\n",
              "\n",
              "## Step 3: Combine the results for the polar coordinates\n",
              "Therefore, the polar coordinates of the point $(0, 3)$ are $\\left(3, \\frac{\\pi}{2}\\right)$.\n",
              "\n",
              "The final answer is: $\\boxed{\\left(3, \\frac{\\pi}{2}\\right)}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uCpYzAw_4o9"
      },
      "source": [
        "## 3. Assembling It All! ğŸ§‘â€ğŸ­ï¸\n",
        "\n",
        "Now, let's create a method that encapsulates the entire pipeline. This will allow us to easily reuse the process in future applications, making it efficient and modular.\n",
        "\n",
        "By combining the LLM, PRM, search strategy, and result display, we can simplify the workflow and ensure that itâ€™s reusable for other tasks or questions.\n",
        "\n",
        "We simplify the workflow, ensuring that itâ€™s reusable for different tasks or questions. Additionally, weâ€™ll track the time spent on each method so that we can **understand the practical implications** of using each strategy and configuration.\n",
        "\n",
        "Hereâ€™s how we can structure the method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpswbcVi37KR"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def generate_with_search_and_learn(question, config, llm, prm, method='best_of_n'):\n",
        "    \"\"\"\n",
        "    Generate an answer for a given question using the search-and-learn pipeline.\n",
        "\n",
        "    Args:\n",
        "    - question (str): The input question to generate an answer for.\n",
        "    - config (Config): Configuration object containing parameters for search strategy.\n",
        "    - llm (LLM): Pretrained large language model used for generating answers.\n",
        "    - prm (RLHFFlow): Process reward model used for evaluating answers.\n",
        "    - method (str): Search strategy to use. Options are 'best_of_n', 'beam_search', 'dvts'. Default is 'best_of_n'.\n",
        "\n",
        "    Returns:\n",
        "    - str: The formatted output after processing the question.\n",
        "    \"\"\"\n",
        "    batch = {\"problem\": [question]}\n",
        "\n",
        "    start_time = time.time()\n",
        "    if method == 'best_of_n':\n",
        "      result = best_of_n(x=batch, config=config, llm=llm, prm=prm)\n",
        "    elif method == 'beam_search':\n",
        "      result = beam_search(examples=batch, config=config, llm=llm, prm=prm)\n",
        "    elif method == 'dvts':\n",
        "      result = dvts(examples=batch, config=config, llm=llm, prm=prm)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"\\nFinished in {elapsed_time:.2f} seconds\\n\")\n",
        "\n",
        "    tokenizer = llm.get_tokenizer()\n",
        "    total_tokens = 0\n",
        "    for completion in result['completions']:\n",
        "        for comp in  completion:\n",
        "            output_tokens = tokenizer.encode(comp)\n",
        "            total_tokens += len(output_tokens)\n",
        "\n",
        "    print(f\"Total tokens in all completions: {total_tokens}\")\n",
        "\n",
        "    formatted_output = result['pred'][0].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\").strip()\n",
        "    return formatted_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWbOqkiKPVd2"
      },
      "source": [
        "### â³  3.1 Comparing Thinking Time for Each Strategy\n",
        "\n",
        "Letâ€™s compare the **thinking time** of three methods: `best_of_n`, `beam_search`, and `dvts`. Each method is evaluated using the same number of answers during the search process, measuring the time spent thinking in seconds and the number of generated tokens.\n",
        "\n",
        "In the results below, the `best_of_n` method shows the least thinking time, while the `dvts` method takes the most time. However, `best_of_n` generates more tokens due to its simpler search strategy.\n",
        "\n",
        "| **Method**      | **Number of Answers During Search** | **Thinking Time (Seconds)** | **Generated Tokens** |\n",
        "|------------------|-------------------------------------|-----------------------------|-----------------------|\n",
        "| **best_of_n**    | 8                                   | 3.54                        | 3087                  |\n",
        "| **beam_search**  | 8                                   | 10.06                       | 2049                  |\n",
        "| **dvts**         | 8                                   | 8.46                        | 2544                  |\n",
        "\n",
        "This comparison illustrates the trade-offs between the strategies, balancing time spent thinking and the complexity of the search process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ROJwROGX8q-"
      },
      "source": [
        "#### 1. **Best of n**\n",
        "\n",
        "Weâ€™ll begin by using the `best_of_n` strategy. Hereâ€™s how to track the thinking time for this method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_fWKy5CCTLV",
        "outputId": "8d77eea3-b23e-4eba-cfe3-5935fae1405d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 3.54 seconds\n",
            "\n",
            "Total tokens in all completions: 3087\n"
          ]
        }
      ],
      "source": [
        "question = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'\n",
        "\n",
        "config.n=8\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='best_of_n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "uzKfFoKG9ejC",
        "outputId": "38326907-685e-4a9c-ca8b-32a7c40f1d3e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: Recall the conversion formulas from rectangular to polar coordinates\n",
              "The conversion formulas are $r = \\sqrt{x^2 + y^2}$ for the radial coordinate and $\\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)$ for the angular coordinate.\n",
              "\n",
              "## Step 2: Substitute the given rectangular coordinates into the formulas\n",
              "Given the point $(0, 3)$, we substitute $x = 0$ and $y = 3$ into the formulas.\n",
              "\n",
              "## Step 3: Calculate the radial coordinate\n",
              "$r = \\sqrt{0^2 + 3^2} = \\sqrt{0 + 9} = \\sqrt{9} = 3$\n",
              "\n",
              "## Step 4: Calculate the angular coordinate\n",
              "$\\theta = \\tan^{-1}\\left(\\frac{3}{0}\\right) = \\tan^{-1}(\\infty) = \\frac{\\pi}{2}$\n",
              "\n",
              "## Step 5: Combine the results\n",
              "The polar coordinates of the point $(0, 3)$ are $\\left(3, \\frac{\\pi}{2}\\right)$.\n",
              "\n",
              "The final answer is: $\\boxed{\\left(3, \\frac{\\pi}{2}\\right)}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S9AwP5lQvUN"
      },
      "source": [
        "#### 2. **Beam Search**\n",
        "\n",
        "Now, let's try using the `beam_search` strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7CH6KN8Izp9",
        "outputId": "adef4782-3278-4994-9520-43e23ea047a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Beam search iterations:  20%|â–ˆâ–ˆ        | 8/40 [00:10<00:40,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 10.06 seconds\n",
            "\n",
            "Total tokens in all completions: 2049\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config.n=8\n",
        "# beam search specific\n",
        "config.sort_completed=True\n",
        "config.filter_duplicates=True\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='beam_search')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "Hw6tQD_dMwXZ",
        "outputId": "0f66c7ed-2071-45a4-e562-3967deb0bc9d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: To convert the point (0,3) from rectangular coordinates to polar coordinates, we need to find the radius (r) and the angle (heta).\n",
              "\n",
              "The formula to convert from rectangular coordinates (x, y) to polar coordinates (r, heta) is given by:\n",
              "r = sqrt(x^2 + y^2)\n",
              "heta = atan2(y, x)\n",
              "\n",
              "## Step 2: Plug in the values (0,3) into the formula to find the radius (r).\n",
              "\n",
              "r = sqrt(0^2 + 3^2)\n",
              "r = sqrt(0 + 9)\n",
              "r = sqrt(9)\n",
              "r = 3\n",
              "\n",
              "## Step 3: Plug in the values (0,3) into the formula to find the angle (heta).\n",
              "\n",
              "heta = atan2(3, 0)\n",
              "Since the point (0,3) is in the first quadrant and lies on the positive y-axis, heta = pi/2 (or 90 degrees).\n",
              "\n",
              "## Step 4: Combine r and heta to get the polar coordinates.\n",
              "\n",
              "The polar coordinates are (3, pi/2).\n",
              "\n",
              "The final answer is: $\\boxed{(3, \\frac{\\pi}{2})}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxBBUd7HQzhd"
      },
      "source": [
        "#### 3. **Diverse Verifier Tree Search (DVTS)**\n",
        "\n",
        "Finally, let's try the `dvts` strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzXW1g-dI5wN",
        "outputId": "86979d67-7dfa-4346-9adb-c386a52af58c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Beam search iterations:  22%|â–ˆâ–ˆâ–       | 9/40 [00:08<00:29,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 8.46 seconds\n",
            "\n",
            "Total tokens in all completions: 2544\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config.n=8\n",
        "# dvts specific\n",
        "config.n_beams = config.n // config.beam_width\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='dvts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "RGkG9MPXMvN0",
        "outputId": "18a333ae-7b3a-455e-df2c-bb497b1381a5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: To convert the point (0,3) from rectangular coordinates to polar coordinates, we need to find the radius r and the angle theta.\n",
              "\n",
              "The radius r can be calculated using the formula $r = \\sqrt{x^2 + y^2}$, where x is the x-coordinate and y is the y-coordinate.\n",
              "\n",
              "## Step 2: Substitute the values of x and y into the formula to find the radius r.\n",
              "\n",
              "$r = \\sqrt{0^2 + 3^2}$\n",
              "$r = \\sqrt{9}$\n",
              "$r = 3$\n",
              "\n",
              "## Step 3: Now that we have the radius r, we can find the angle theta using the formula $\\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)$.\n",
              "\n",
              "Since x = 0 and y = 3, the angle theta is 90 degrees or $\\frac{\\pi}{2}$ radians.\n",
              "\n",
              "## Step 4: Now that we have the radius r and the angle theta, we can write the polar coordinates as (r, theta).\n",
              "\n",
              "Therefore, the polar coordinates for the point (0, 3) are $\\left(3, \\frac{\\pi}{2}\\right).$\n",
              "\n",
              "The final answer is: $\\boxed{\\left(3, \\frac{\\pi}{2}\\right)}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PM9HHwBSYWk"
      },
      "source": [
        "### ğŸ™‹ 3.2 Testing the System with a Simple Question\n",
        "\n",
        "In this final example, weâ€™ll test the system using a straightforward question to observe how it performs in simpler cases. This allows us to verify that the system works as expected even for basic queries.\n",
        "\n",
        "Let's try the following question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq9vM1uRM7A8",
        "outputId": "65ef318d-2b89-4d46-b660-293195c2b8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 1.03 seconds\n",
            "\n",
            "Total tokens in all completions: 544\n"
          ]
        }
      ],
      "source": [
        "question = 'What\\'s the capital of Spain?'\n",
        "\n",
        "config.n=32\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='best_of_n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "ysfR0nPfM-Ub",
        "outputId": "b474aeb6-6cb7-4f15-ba48-fa59022f31ef"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The capital of Spain is Madrid."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgdeSegeANoT"
      },
      "source": [
        "Even though we set a larger number of candidate answers (`N`), the time spent thinking remains relatively small (1.03 seconds and 544 generated tokens). This demonstrates the systemâ€™s ability to efficiently handle easier problems, spending less time on them, while leveraging its enhanced capabilities for more complex questions.\n",
        "\n",
        "ğŸ† **We now have a fully operational pipeline** that leverages test-time compute, enabling the system to \"think longer\" for more complicated queries, while also maintaining fast response times for straightforward questions.\n",
        "\n",
        "This approach ensures the system can scale its thinking time based on the task's complexity, offering an efficient and responsive solution for both simple and challenging problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92znAyJ0AOPY"
      },
      "source": [
        "## 4. Continuing the Journey and Resources ğŸ§‘â€ğŸ“ï¸\n",
        "\n",
        "If you're eager to continue exploring, be sure to check out the original experimental [blog](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute) and all the references mentioned within it. These resources will deepen your understanding of test-time compute, its benefits, and its applications in LLMs.\n",
        "\n",
        "\n",
        "Happy learning and experimenting! ğŸš€"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}